{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37428026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingn\\CPSY1291\\cpsy-1291-final-project\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "098abb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv('OPENROUTER_API_KEY'),\n",
    "    # api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f18201ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Um — do you know where the world’s largest ice sheet is located today?\"\n",
    "\n",
    "classifier_input = f'''Please assess what personality best fits the following text. The categories are:  \n",
    "- Formal  \n",
    "- Casual  \n",
    "- Confident  \n",
    "- Hesitant  \n",
    "- Analytical  \n",
    "- Emotional  \n",
    "- Optimistic  \n",
    "- Pessimistic  \n",
    "\n",
    "<text>  \n",
    "{user_input}\n",
    "</text>  \n",
    "\n",
    "Please respond with a single word.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cf992",
   "metadata": {},
   "source": [
    "### Simple Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8938945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='google/gemini-2.5-flash',\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": classifier_input,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78412aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hesitant\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca21205",
   "metadata": {},
   "source": [
    "### Classifier with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8fe06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='google/gemini-2.5-flash',\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": classifier_input,\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdde2df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hesitant\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6547c6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hesitant', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='STOP')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "896d8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03d967af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'H',\n",
       " 'bytes': [72],\n",
       " 'logprob': -4.320199877838604e-07,\n",
       " 'top_logprobs': [TopLogprob(token='H', bytes=[72], logprob=-4.320199877838604e-07),\n",
       "  TopLogprob(token='Cur', bytes=[67, 117, 114], logprob=-15.25),\n",
       "  TopLogprob(token='Cas', bytes=[67, 97, 115], logprob=-15.75),\n",
       "  TopLogprob(token=' hesitant', bytes=[32, 104, 101, 115, 105, 116, 97, 110, 116], logprob=-17.375),\n",
       "  TopLogprob(token='P', bytes=[80], logprob=-17.75)]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0c181bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='H', bytes=[72], logprob=-4.320199877838604e-07),\n",
       " TopLogprob(token='Cur', bytes=[67, 117, 114], logprob=-15.25),\n",
       " TopLogprob(token='Cas', bytes=[67, 97, 115], logprob=-15.75),\n",
       " TopLogprob(token=' hesitant', bytes=[32, 104, 101, 115, 105, 116, 97, 110, 116], logprob=-17.375),\n",
       " TopLogprob(token='P', bytes=[80], logprob=-17.75)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb9fec",
   "metadata": {},
   "source": [
    "### Classifier with logits – using GitHub models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c974b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge model -- assessing logprobs from different models to determine max persona\n",
    "\n",
    "def get_model_completion(model_input: str, model: str = 'openai/gpt-4.1-mini'): \n",
    "    url = \"https://models.github.ai/inference/chat/completions\"\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": model_input\n",
    "            }\n",
    "        ],\n",
    "        \"logprobs\": True, \n",
    "        \"top_logprobs\": 5,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "    completion = json.loads(resp.text)\n",
    "\n",
    "    return completion, resp\n",
    "\n",
    "\n",
    "def print_rate_limits(response):\n",
    "    print(f'total rate limit requests per hour: {response.headers['x-ratelimit-limit-requests']}')\n",
    "    print(f'rate limit requests remaining this hour: {response.headers['x-ratelimit-remaining-requests']}')\n",
    "\n",
    "    print(f'total rate limit tokens per hour: {response.headers['x-ratelimit-limit-tokens']}')\n",
    "    print(f'rate limit tokens remaining this hour: {response.headers['x-ratelimit-remaining-tokens']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90fd8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion, response = get_model_completion(model_input=classifier_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70649bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations': [],\n",
       " 'content': 'Hesitant',\n",
       " 'refusal': None,\n",
       " 'role': 'assistant'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = completion['choices'][0]['message']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e8679c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bytes': [72], 'logprob': -9.088346359931165e-07, 'token': 'H'},\n",
       " {'bytes': [67, 97, 115], 'logprob': -14.500000953674316, 'token': 'Cas'},\n",
       " {'bytes': [104, 101, 115], 'logprob': -14.750000953674316, 'token': 'hes'},\n",
       " {'bytes': [32, 104, 101, 115, 105, 116, 97, 110, 116],\n",
       "  'logprob': -20.375,\n",
       "  'token': ' hesitant'},\n",
       " {'bytes': [32, 72, 101, 115], 'logprob': -20.5, 'token': ' Hes'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logprobs = completion['choices'][0]['logprobs']['content']\n",
    "# assert len(all_logprobs) == 1   # ie. the model should respond with a single token\n",
    "\n",
    "all_logprobs[0]['top_logprobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8f1dd",
   "metadata": {},
   "source": [
    "### Getting Output from 2 Models -- Meta, OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21571ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2 models via GitHub\n",
    "MODELS = {\n",
    "    \"Meta Llama 3.1 8B\": \"meta/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"OpenAI gpt-4.1-mini\": \"gpt-4.1-mini\",\n",
    "    # \"Google Gemma 3 4B\": \"gemma-3-4b-instruct\",\n",
    "}\n",
    "\n",
    "def get_github_model_response(model_name, model_id, prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Get response from GitHub model using direct API.\n",
    "    Returns dict with status, model name, response, and any errors.\n",
    "    \"\"\"\n",
    "    url = \"https://models.github.ai/inference/chat/completions\"\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_id,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "        completion = resp.json()\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            message = completion['choices'][0]['message']['content']\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"status\": \"success\",\n",
    "                \"response\": message,\n",
    "            }\n",
    "        else:\n",
    "            error_msg = completion.get('error', {}).get('message', f'HTTP {resp.status_code}')\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": error_msg,\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        status = \"error\"\n",
    "        if \"429\" in error_msg or \"rate\" in error_msg.lower():\n",
    "            status = \"rate_limited\"\n",
    "        elif \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "            status = \"auth_error\"\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"status\": status,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_all_github_model_responses(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Get responses from all GitHub models in sequence.\n",
    "    Returns list of response dicts.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for model_name, model_id in MODELS.items():\n",
    "        print(f\"\\nCalling {model_name}...\")\n",
    "        response = get_github_model_response(model_name, model_id, prompt, max_tokens)\n",
    "        responses.append(response)\n",
    "        \n",
    "        if response[\"status\"] == \"success\":\n",
    "            resp_text = response['response'].strip() if response['response'] else \"(empty)\"\n",
    "            print(f\"✓ {model_name}: {resp_text[:100]}\")\n",
    "        else:\n",
    "            print(f\"✗ {model_name}: {response['status']} - {response.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65071b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling Meta Llama 3.1 8B...\n",
      "✓ Meta Llama 3.1 8B: It can be tough to make decisions, especially when you're not sure about the outcome. Would you like\n",
      "\n",
      "Calling OpenAI gpt-4.1-mini...\n",
      "✓ OpenAI gpt-4.1-mini: It sounds like you’re feeling uncertain. If you’d like, you can share more about the decision you’re\n",
      "\n",
      "Meta Llama 3.1 8B:\n",
      "  Status: success\n",
      "  Response: It can be tough to make decisions, especially when you're not sure about the outcome. Would you like to talk more about what's on your mind and what's making you unsure? Sometimes sharing your thoughts and feelings with someone (or in this case, a conversational AI) can help you clarify your thoughts and gain a new perspective. What's going on?\n",
      "\n",
      "OpenAI gpt-4.1-mini:\n",
      "  Status: success\n",
      "  Response: It sounds like you’re feeling uncertain. If you’d like, you can share more about the decision you’re facing, and I can help you think through the options.\n"
     ]
    }
   ],
   "source": [
    "# Get responses from models\n",
    "user_input = \"I'm not sure about this decision.\"\n",
    "responses = get_all_github_model_responses(user_input, max_tokens=100)\n",
    "# Display results\n",
    "for resp in responses:\n",
    "    print(f\"\\n{resp['model']}:\")\n",
    "    print(f\"  Status: {resp['status']}\")\n",
    "    if resp['status'] == 'success':\n",
    "        print(f\"  Response: {resp['response']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {resp.get('error', 'N/A')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
