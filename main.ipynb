{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37428026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cpsy-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098abb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# OpenRouter client for model access\n",
    "openrouter_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18201ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Um — do you know where the world’s largest ice sheet is located today?\"\n",
    "\n",
    "classifier_input = f'''Please assess what personality best fits the following text. The categories are:  \n",
    "- Formal  \n",
    "- Casual  \n",
    "- Confident  \n",
    "- Hesitant  \n",
    "- Analytical  \n",
    "- Emotional  \n",
    "- Optimistic  \n",
    "- Pessimistic  \n",
    "\n",
    "<text>  \n",
    "{user_input}\n",
    "</text>  \n",
    "\n",
    "Please respond with a single word.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cf992",
   "metadata": {},
   "source": [
    "### Simple Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='google/gemini-2.5-flash',\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": classifier_input,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78412aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca21205",
   "metadata": {},
   "source": [
    "### Classifier with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fe06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='google/gemini-2.5-flash',\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": classifier_input,\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d967af",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c181bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb9fec",
   "metadata": {},
   "source": [
    "### Classifier with logits – using GitHub models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c974b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge model -- assessing logprobs from different models to determine max persona\n",
    "\n",
    "def get_model_completion(model_input: str, model: str = 'openai/gpt-4.1-mini'): \n",
    "    url = \"https://models.github.ai/inference/chat/completions\"\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": model_input\n",
    "            }\n",
    "        ],\n",
    "        \"logprobs\": True, \n",
    "        \"top_logprobs\": 5,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "    completion = json.loads(resp.text)\n",
    "\n",
    "    return completion, resp\n",
    "\n",
    "\n",
    "def print_rate_limits(response):\n",
    "    print(f'total rate limit requests per hour: {response.headers['x-ratelimit-limit-requests']}')\n",
    "    print(f'rate limit requests remaining this hour: {response.headers['x-ratelimit-remaining-requests']}')\n",
    "\n",
    "    print(f'total rate limit tokens per hour: {response.headers['x-ratelimit-limit-tokens']}')\n",
    "    print(f'rate limit tokens remaining this hour: {response.headers['x-ratelimit-remaining-tokens']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion, response = get_model_completion(model_input=classifier_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70649bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = completion['choices'][0]['message']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8679c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logprobs = completion['choices'][0]['logprobs']['content']\n",
    "# assert len(all_logprobs) == 1   # ie. the model should respond with a single token\n",
    "\n",
    "all_logprobs[0]['top_logprobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8f1dd",
   "metadata": {},
   "source": [
    "### Getting Output from 2 Models -- Meta, OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21571ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter models\n",
    "MODELS = {\n",
    "    \"Meta Llama 3.1 8B\": \"meta-llama/llama-3.1-8b-instruct\",\n",
    "    \"OpenAI gpt-4.1-mini\": \"openai/gpt-4-turbo\",\n",
    "    \"DeepSeek V3\": \"deepseek/deepseek-chat\",\n",
    "}\n",
    "\n",
    "def get_github_model_response(model_id, prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Get response from OpenRouter model using OpenAI client.\n",
    "    Returns dict with status, model name, response, and any errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openrouter_client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message.content\n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"status\": \"success\",\n",
    "            \"response\": message,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        status = \"error\"\n",
    "        \n",
    "        if \"429\" in error_msg or \"rate\" in error_msg.lower():\n",
    "            status = \"rate_limited\"\n",
    "        elif \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "            status = \"auth_error\"\n",
    "        elif \"insufficient_quota\" in error_msg.lower():\n",
    "            status = \"insufficient_quota\"\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"status\": status,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_all_github_model_responses(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Get responses from all OpenRouter models in sequence.\n",
    "    Returns list of response dicts.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for model_name, model_id in MODELS.items():\n",
    "        print(f\"\\nCalling {model_name}...\")\n",
    "        response = get_github_model_response(model_id, prompt, max_tokens)\n",
    "        responses.append(response)\n",
    "        \n",
    "        if response[\"status\"] == \"success\":\n",
    "            resp_text = response['response'].strip() if response['response'] else \"(empty)\"\n",
    "            print(f\"{model_name}: {resp_text[:100]}\")\n",
    "        else:\n",
    "            print(f\"{model_name}: {response['status']} - {response.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65071b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling Meta Llama 3.1 8B...\n",
      "✓ Meta Llama 3.1 8B: It can be really tough to make decisions when you're not feeling confident about the outcome. Would \n",
      "\n",
      "Calling OpenAI gpt-4.1-mini...\n",
      "✓ OpenAI gpt-4.1-mini: It's completely normal to feel unsure when faced with a decision. To help you make a choice you feel\n",
      "\n",
      "Calling DeepSeek V3...\n",
      "✓ DeepSeek V3: It’s completely normal to feel unsure about a decision, especially if it’s an important one. Here ar\n",
      "\n",
      "Meta Llama 3.1 8B:\n",
      "  Status: success\n",
      "  Response: It can be really tough to make decisions when you're not feeling confident about the outcome. Would you like to talk about what's making you unsure and what's at stake in this decision? Sometimes sharing your thoughts and concerns can help you clarify your feelings and think more clearly about what to do. I'm here to listen and offer support if you'd like!\n",
      "\n",
      "OpenAI gpt-4.1-mini:\n",
      "  Status: success\n",
      "  Response: It's completely normal to feel unsure when faced with a decision. To help you make a choice you feel confident about, consider the following steps:\n",
      "\n",
      "1. **Define the Decision**: What exactly is the decision you need to make? Understanding the decision in clear terms can help make the process less overwhelming.\n",
      "\n",
      "2. **Gather Information**: Collect all the relevant information and data that will impact your decision. This could involve researching, talking to knowledgeable people, or seeking expert advice.\n",
      "\n",
      "3. **Identify\n",
      "\n",
      "DeepSeek V3:\n",
      "  Status: success\n",
      "  Response: It’s completely normal to feel unsure about a decision, especially if it’s an important one. Here are a few steps you can take to gain clarity:\n",
      "\n",
      "1. **Pause and Reflect**: Take a moment to breathe and step back from the situation. This can help you gain perspective and reduce any immediate pressure.\n",
      "\n",
      "2. **Identify Your Concerns**: Write down what exactly is making you unsure. Are there specific risks, unknowns, or conflicting priorities?\n",
      "\n",
      "3. **Gather Information**: If\n"
     ]
    }
   ],
   "source": [
    "# Get responses from models\n",
    "user_input = \"I'm not sure about this decision.\"\n",
    "responses = get_all_github_model_responses(user_input, max_tokens=100)\n",
    "# Display results\n",
    "for resp in responses:\n",
    "    print(f\"\\n{resp['model']}:\")\n",
    "    print(f\"  Status: {resp['status']}\")\n",
    "    if resp['status'] == 'success':\n",
    "        print(f\"  Response: {resp['response']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {resp.get('error', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282a796",
   "metadata": {},
   "source": [
    "### Process All Questions from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70fb0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_questions_with_model(model_id, sleep_duration=1.0, question_indexes=None):\n",
    "    \"\"\"\n",
    "    Process questions from database, send to model, and store responses.\n",
    "    \n",
    "    Output dictionary structure:\n",
    "        {\n",
    "            'Control': [{'prompt': '...', 'response': '...'}, ...],\n",
    "            'Formal': [{'prompt': '...', 'response': '...'}, ...],\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    emotion_categories = [\n",
    "        'Control', 'Formal', 'Casual', 'Confident', \n",
    "        'Hesitant', 'Analytical', 'Emotional', 'Optimistic', 'Pessimistic'\n",
    "    ]\n",
    "    \n",
    "    results = {category: [] for category in emotion_categories}\n",
    "    \n",
    "    with open('questions_database.json', 'r') as f:\n",
    "        questions_db = json.load(f)\n",
    "    \n",
    "    if question_indexes is not None:\n",
    "        questions_db = questions_db[question_indexes[0]:question_indexes[1]]\n",
    "    \n",
    "    total_requests = len(questions_db) * len(emotion_categories)\n",
    "    print(f\"Processing {len(questions_db)} questions with {len(emotion_categories)} emotion categories\")\n",
    "    print(f\"Total API calls: {total_requests}\")\n",
    "    print(f\"Estimated time: {total_requests * sleep_duration / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    for q_idx, question_entry in enumerate(tqdm(questions_db, desc=\"Questions\")):\n",
    "        for category in emotion_categories:\n",
    "            if category not in question_entry:\n",
    "                print(f\"Warning: Category '{category}' not found in question {q_idx}\")\n",
    "                continue\n",
    "            \n",
    "            prompt = question_entry[category]\n",
    "            \n",
    "            response_data = get_github_model_response(\n",
    "                model_id=model_id,\n",
    "                prompt=prompt,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            if response_data['status'] == 'success':\n",
    "                results[category].append({\n",
    "                    'prompt': prompt,\n",
    "                    'response': response_data['response']\n",
    "                })\n",
    "            else:\n",
    "                error_msg = response_data.get('error', 'Unknown error')\n",
    "                print(f\"\\nError for question {q_idx}, category {category}: {error_msg}\")\n",
    "                results[category].append({\n",
    "                    'prompt': prompt,\n",
    "                    'response': None,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "            \n",
    "            time.sleep(sleep_duration)\n",
    "    \n",
    "    print(f\"\\nCompleted processing!\")\n",
    "    print(f\"Results summary:\")\n",
    "    for category, entries in results.items():\n",
    "        successful = sum(1 for e in entries if e.get('response') is not None)\n",
    "        print(f\"  {category}: {successful}/{len(entries)} successful\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c9e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 2 questions for all models\n",
      "Processing 2 questions with 9 emotion categories\n",
      "Total API calls: 18\n",
      "Estimated time: 1.5 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|██████████| 2/2 [02:31<00:00, 75.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed processing!\n",
      "Results summary:\n",
      "  Control: 2/2 successful\n",
      "  Formal: 2/2 successful\n",
      "  Casual: 2/2 successful\n",
      "  Confident: 2/2 successful\n",
      "  Hesitant: 2/2 successful\n",
      "  Analytical: 2/2 successful\n",
      "  Emotional: 2/2 successful\n",
      "  Optimistic: 2/2 successful\n",
      "  Pessimistic: 2/2 successful\n",
      "Processing 2 questions with 9 emotion categories\n",
      "Total API calls: 18\n",
      "Estimated time: 1.5 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|██████████| 2/2 [02:17<00:00, 68.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed processing!\n",
      "Results summary:\n",
      "  Control: 2/2 successful\n",
      "  Formal: 2/2 successful\n",
      "  Casual: 2/2 successful\n",
      "  Confident: 2/2 successful\n",
      "  Hesitant: 2/2 successful\n",
      "  Analytical: 2/2 successful\n",
      "  Emotional: 2/2 successful\n",
      "  Optimistic: 2/2 successful\n",
      "  Pessimistic: 2/2 successful\n",
      "Processing 2 questions with 9 emotion categories\n",
      "Total API calls: 18\n",
      "Estimated time: 1.5 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|██████████| 2/2 [02:40<00:00, 80.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed processing!\n",
      "Results summary:\n",
      "  Control: 2/2 successful\n",
      "  Formal: 2/2 successful\n",
      "  Casual: 2/2 successful\n",
      "  Confident: 2/2 successful\n",
      "  Hesitant: 2/2 successful\n",
      "  Analytical: 2/2 successful\n",
      "  Emotional: 2/2 successful\n",
      "  Optimistic: 2/2 successful\n",
      "  Pessimistic: 2/2 successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing with 2 questions for all models\")\n",
    "deepseek_results = process_questions_with_model(\n",
    "    model_id=MODELS[\"DeepSeek V3\"], \n",
    "    sleep_duration=5, \n",
    "    question_indexes=(0, 2)\n",
    ")\n",
    "with open(f'responses_deepseek.json', 'w') as f:\n",
    "    json.dump(deepseek_results, f, indent=2)\n",
    "\n",
    "llama_results = process_questions_with_model(\n",
    "    model_id=MODELS[\"Meta Llama 3.1 8B\"], \n",
    "    sleep_duration=5, \n",
    "    question_indexes=(0, 2)\n",
    ")\n",
    "with open(f'responses_llama.json', 'w') as f:\n",
    "    json.dump(llama_results, f, indent=2)\n",
    "\n",
    "gpt_results = process_questions_with_model(\n",
    "    model_id=MODELS[\"OpenAI gpt-4.1-mini\"], \n",
    "    sleep_duration=5,\n",
    "    question_indexes=(0, 2)\n",
    ")\n",
    "with open(f'responses_gpt4.json', 'w') as f:\n",
    "    json.dump(gpt_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4e2bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_INDEX = 2\n",
    "WINDOW_SZ = 5\n",
    "SLEEP_DURATION = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cb301be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all models for questions in range  (2, 7)\n",
      "Processing 5 questions with 9 emotion categories\n",
      "Total API calls: 45\n",
      "Estimated time: 1.9 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|██████████| 5/5 [03:54<00:00, 46.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed processing!\n",
      "Results summary:\n",
      "  Control: 5/5 successful\n",
      "  Formal: 5/5 successful\n",
      "  Casual: 5/5 successful\n",
      "  Confident: 5/5 successful\n",
      "  Hesitant: 5/5 successful\n",
      "  Analytical: 5/5 successful\n",
      "  Emotional: 5/5 successful\n",
      "  Optimistic: 5/5 successful\n",
      "  Pessimistic: 5/5 successful\n",
      "✓ Merged with existing data from responses_llama.json\n",
      "✓ Saved 63 total responses\n",
      "Processing 5 questions with 9 emotion categories\n",
      "Total API calls: 45\n",
      "Estimated time: 1.9 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions:  80%|████████  | 4/5 [04:56<01:14, 74.22s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIStatusError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget_github_model_response\u001b[39m\u001b[34m(model_id, prompt, max_tokens)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     response = \u001b[43mopenrouter_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     message = response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cpsy-env/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cpsy-env/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1191\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cpsy-env/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/cpsy-env/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mAPIStatusError\u001b[39m: Error code: 402 - {'error': {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 150 tokens, but can only afford 118. To increase, visit https://openrouter.ai/settings/credits and upgrade to a paid account', 'code': 402, 'metadata': {'provider_name': None}}, 'user_id': 'user_36a2F29peJjDr1kJxZ2iBnMa07a'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m model_files = {\n\u001b[32m      4\u001b[39m     MODELS[\u001b[33m\"\u001b[39m\u001b[33mMeta Llama 3.1 8B\u001b[39m\u001b[33m\"\u001b[39m]: \u001b[33m\"\u001b[39m\u001b[33mresponses_llama.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     MODELS[\u001b[33m\"\u001b[39m\u001b[33mOpenAI gpt-4.1-mini\u001b[39m\u001b[33m\"\u001b[39m]: \u001b[33m\"\u001b[39m\u001b[33mresponses_gpt4.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     MODELS[\u001b[33m\"\u001b[39m\u001b[33mDeepSeek V3\u001b[39m\u001b[33m\"\u001b[39m]: \u001b[33m\"\u001b[39m\u001b[33mresponses_deepseek.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m }\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_id, filename \u001b[38;5;129;01min\u001b[39;00m model_files.items():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     results = \u001b[43mprocess_questions_with_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep_duration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSLEEP_DURATION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_indexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTART_INDEX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_INDEX\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mWINDOW_SZ\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Merge\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(filename):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mprocess_questions_with_model\u001b[39m\u001b[34m(model_id, sleep_duration, question_indexes)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     38\u001b[39m prompt = question_entry[category]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m response_data = \u001b[43mget_github_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_data[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     47\u001b[39m     results[category].append({\n\u001b[32m     48\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m: prompt,\n\u001b[32m     49\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m: response_data[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     50\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mget_github_model_response\u001b[39m\u001b[34m(model_id, prompt, max_tokens)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minsufficient_quota\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg.lower():\n\u001b[32m     41\u001b[39m     status = \u001b[33m\"\u001b[39m\u001b[33minsufficient_quota\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mmodel_name\u001b[49m,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_id\u001b[39m\u001b[33m\"\u001b[39m: model_id,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: status,\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: error_msg,\n\u001b[32m     48\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Processing all models for questions in range \", (START_INDEX, START_INDEX + WINDOW_SZ))\n",
    "\n",
    "model_files = {\n",
    "    MODELS[\"Meta Llama 3.1 8B\"]: \"responses_llama.json\",\n",
    "    MODELS[\"OpenAI gpt-4.1-mini\"]: \"responses_gpt4.json\",\n",
    "    MODELS[\"DeepSeek V3\"]: \"responses_deepseek.json\",\n",
    "}\n",
    "\n",
    "for model_id, filename in model_files.items():\n",
    "    print(f\"\\nProcessing model {model_id}...\")\n",
    "    results = process_questions_with_model(model_id=model_id, sleep_duration=SLEEP_DURATION, question_indexes=(START_INDEX, START_INDEX + WINDOW_SZ))\n",
    "\n",
    "    # Merge\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            existing_data = json.load(f)\n",
    "        for category in results.keys():\n",
    "            existing_data[category].extend(results[category])\n",
    "        results = existing_data\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de862d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fixed Unicode escape sequences in results_meta_Meta-Llama-3.1-8B-Instruct.json\n"
     ]
    }
   ],
   "source": [
    "def fix_unicode_in_json_file(filepath):\n",
    "    \"\"\"\n",
    "    Load JSON file and resave it with actual Unicode characters instead of escape sequences.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Fixed Unicode escape sequences in {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ee21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - uncomment to run:\n",
    "# deepseek_results = process_deepseek_questions(\n",
    "#     max_daily_requests=50,\n",
    "#     sleep_duration=6.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! NOT USING IT \n",
    "\n",
    "def process_deepseek_questions(max_daily_requests=50, sleep_duration=1.0, progress_file='deepseek_progress.json', output_file='responses_deepseek.json'):\n",
    "    \"\"\"\n",
    "    Process questions with DeepSeek model via OpenRouter.\n",
    "    \n",
    "    Args:\n",
    "        max_daily_requests: Maximum API calls per day (default 50)\n",
    "        sleep_duration: Seconds between requests (default 1.0)\n",
    "        progress_file: Path to save/load progress\n",
    "        output_file: Path to save final results\n",
    "    \n",
    "    Returns:\n",
    "        Array of question objects with all emotion categories\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    EMOTION_CATEGORIES = [\n",
    "        'Control', 'Formal', 'Casual', 'Confident',\n",
    "        'Hesitant', 'Analytical', 'Emotional', 'Optimistic', 'Pessimistic'\n",
    "    ]\n",
    "    \n",
    "    def load_progress():\n",
    "        try:\n",
    "            if os.path.exists(progress_file):\n",
    "                with open(progress_file, 'r') as f:\n",
    "                    progress = json.load(f)\n",
    "                print(f\"Resuming from question {progress['questionIndex']}, category {progress['categoryIndex']}\")\n",
    "                return progress\n",
    "        except Exception as err:\n",
    "            print(f'Could not load progress file: {err}')\n",
    "        return {'questionIndex': 0, 'categoryIndex': 0, 'totalRequests': 0, 'results': []}\n",
    "    \n",
    "    def save_progress(question_index, category_index, total_requests, results):\n",
    "        progress = {\n",
    "            'questionIndex': question_index,\n",
    "            'categoryIndex': category_index,\n",
    "            'totalRequests': total_requests,\n",
    "            'results': results\n",
    "        }\n",
    "        with open(progress_file, 'w') as f:\n",
    "            json.dump(progress, f, indent=2)\n",
    "    \n",
    "    def save_results(results):\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f'Results saved to {output_file}')\n",
    "    \n",
    "    # Load questions database\n",
    "    with open('questions_database.json', 'r') as f:\n",
    "        questions_db = json.load(f)\n",
    "    \n",
    "    # Load or initialize progress\n",
    "    progress = load_progress()\n",
    "    question_index = progress['questionIndex']\n",
    "    category_index = progress['categoryIndex']\n",
    "    total_requests = progress['totalRequests']\n",
    "    results = progress['results']\n",
    "    \n",
    "    # Initialize results as array if empty\n",
    "    if not isinstance(results, list) or len(results) == 0:\n",
    "        results = [] \n",
    "    \n",
    "    try:\n",
    "        # Process each question\n",
    "        for q_idx in range(question_index, len(questions_db)):\n",
    "            question_entry = questions_db[q_idx]\n",
    "            \n",
    "            # Initialize question object if starting a new question\n",
    "            if q_idx >= len(results):\n",
    "                results.append({})\n",
    "            \n",
    "            # Process each emotion category\n",
    "            start_category = category_index if q_idx == question_index else 0\n",
    "            for c_idx in range(start_category, len(EMOTION_CATEGORIES)):\n",
    "                category = EMOTION_CATEGORIES[c_idx]\n",
    "                \n",
    "                # Check daily limit\n",
    "                if total_requests >= max_daily_requests:\n",
    "                    print(f\"\\nDaily limit of {max_daily_requests} requests reached!\")\n",
    "                    print(f\"Stopped at question {q_idx}, category {category}\")\n",
    "                    save_progress(q_idx, c_idx, total_requests, results)\n",
    "                    save_results(results)\n",
    "                    return results\n",
    "                \n",
    "                if category not in question_entry:\n",
    "                    print(f\"Warning: Category '{category}' not found in question {q_idx}\")\n",
    "                    continue\n",
    "                \n",
    "                prompt = question_entry[category]\n",
    "                \n",
    "                print(f\"[{total_requests + 1}/{max_daily_requests}] Q{q_idx + 1}/{len(questions_db)} - {category}\")\n",
    "                \n",
    "                # Use existing get_github_model_response function (now uses OpenRouter)\n",
    "                response_data = get_github_model_response(\n",
    "                    model_name=\"DeepSeek V3\",\n",
    "                    model_id=\"deepseek/deepseek-chat\",\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                \n",
    "                if response_data['status'] == 'success':\n",
    "                    results[q_idx][category] = {\n",
    "                        'prompt': prompt,\n",
    "                        'response': response_data['response']\n",
    "                    }\n",
    "                else:\n",
    "                    error_msg = response_data.get('error', 'Unknown error')\n",
    "                    print(f\"Error: {error_msg}\")\n",
    "                    results[q_idx][category] = {\n",
    "                        'prompt': prompt,\n",
    "                        'response': None,\n",
    "                        'error': error_msg\n",
    "                    }\n",
    "                \n",
    "                total_requests += 1\n",
    "                \n",
    "                # Save progress after each request\n",
    "                save_progress(q_idx, c_idx + 1, total_requests, results)\n",
    "                \n",
    "                # Sleep to respect rate limits (except for the last request)\n",
    "                if total_requests < max_daily_requests and not (q_idx == len(questions_db) - 1 and c_idx == len(EMOTION_CATEGORIES) - 1):\n",
    "                    time.sleep(sleep_duration)\n",
    "        \n",
    "        print(f\"\\nProcessing done for {total_requests} requests\")\n",
    "        save_results(results)\n",
    "        \n",
    "        # Clean up progress file on successful completion\n",
    "        if os.path.exists(progress_file):\n",
    "            os.remove(progress_file)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(f\"\\nFatal error: {err}\")\n",
    "        save_progress(question_index, category_index, total_requests, results)\n",
    "        save_results(results)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4cc3fb",
   "metadata": {},
   "source": [
    "### DeepSeek Processing Function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
