{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37428026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098abb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# login(os.getenv('HF_TOKEN'))\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    # api_key=os.getenv('OPENROUTER_API_KEY'),\n",
    "    api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18201ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Um — do you know where the world’s largest ice sheet is located today?\"\n",
    "\n",
    "classifier_input = f'''Please assess what personality best fits the following text. The categories are:  \n",
    "- Formal  \n",
    "- Casual  \n",
    "- Confident  \n",
    "- Hesitant  \n",
    "- Analytical  \n",
    "- Emotional  \n",
    "- Optimistic  \n",
    "- Pessimistic  \n",
    "\n",
    "<text>  \n",
    "{user_input}\n",
    "</text>  \n",
    "\n",
    "Please respond with a single word.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cf992",
   "metadata": {},
   "source": [
    "### Simple Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='google/gemini-2.5-flash',\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": classifier_input,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78412aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca21205",
   "metadata": {},
   "source": [
    "### Classifier with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fe06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='google/gemini-2.5-flash',\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": classifier_input,\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d967af",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c181bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb9fec",
   "metadata": {},
   "source": [
    "### Classifier with logits – using GitHub models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c974b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge model -- assessing logprobs from different models to determine max persona\n",
    "\n",
    "def get_model_completion(model_input: str, model: str = 'openai/gpt-4.1-mini'): \n",
    "    url = \"https://models.github.ai/inference/chat/completions\"\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": model_input\n",
    "            }\n",
    "        ],\n",
    "        \"logprobs\": True, \n",
    "        \"top_logprobs\": 5,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "    completion = json.loads(resp.text)\n",
    "\n",
    "    return completion, resp\n",
    "\n",
    "\n",
    "def print_rate_limits(response):\n",
    "    print(f'total rate limit requests per hour: {response.headers['x-ratelimit-limit-requests']}')\n",
    "    print(f'rate limit requests remaining this hour: {response.headers['x-ratelimit-remaining-requests']}')\n",
    "\n",
    "    print(f'total rate limit tokens per hour: {response.headers['x-ratelimit-limit-tokens']}')\n",
    "    print(f'rate limit tokens remaining this hour: {response.headers['x-ratelimit-remaining-tokens']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion, response = get_model_completion(model_input=classifier_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70649bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = completion['choices'][0]['message']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8679c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logprobs = completion['choices'][0]['logprobs']['content']\n",
    "# assert len(all_logprobs) == 1   # ie. the model should respond with a single token\n",
    "\n",
    "all_logprobs[0]['top_logprobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8f1dd",
   "metadata": {},
   "source": [
    "### Getting Output from 2 Models -- Meta, OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21571ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2 models via GitHub\n",
    "MODELS = {\n",
    "    \"Meta Llama 3.1 8B\": \"meta/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"OpenAI gpt-4.1-mini\": \"gpt-4.1-mini\",\n",
    "    # \"Google Gemma 3 4B\": \"gemma-3-4b-instruct\",\n",
    "}\n",
    "\n",
    "def get_github_model_response(model_name, model_id, prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Get response from GitHub model using direct API.\n",
    "    Returns dict with status, model name, response, and any errors.\n",
    "    \"\"\"\n",
    "    url = \"https://models.github.ai/inference/chat/completions\"\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_id,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url, json=payload, headers=headers, timeout=30)\n",
    "        completion = resp.json()\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            message = completion['choices'][0]['message']['content']\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"status\": \"success\",\n",
    "                \"response\": message,\n",
    "            }\n",
    "        else:\n",
    "            error_msg = completion.get('error', {}).get('message', f'HTTP {resp.status_code}')\n",
    "            return {\n",
    "                \"model\": model_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": error_msg,\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        status = \"error\"\n",
    "        if \"429\" in error_msg or \"rate\" in error_msg.lower():\n",
    "            status = \"rate_limited\"\n",
    "        elif \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "            status = \"auth_error\"\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"model_id\": model_id,\n",
    "            \"status\": status,\n",
    "            \"error\": error_msg,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_all_github_model_responses(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Get responses from all GitHub models in sequence.\n",
    "    Returns list of response dicts.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for model_name, model_id in MODELS.items():\n",
    "        print(f\"\\nCalling {model_name}...\")\n",
    "        response = get_github_model_response(model_name, model_id, prompt, max_tokens)\n",
    "        responses.append(response)\n",
    "        \n",
    "        if response[\"status\"] == \"success\":\n",
    "            resp_text = response['response'].strip() if response['response'] else \"(empty)\"\n",
    "            print(f\"✓ {model_name}: {resp_text[:100]}\")\n",
    "        else:\n",
    "            print(f\"✗ {model_name}: {response['status']} - {response.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65071b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calling Meta Llama 3.1 8B...\n",
      "✓ Meta Llama 3.1 8B: Could you please provide more context or information about the decision you're considering? What are\n",
      "\n",
      "Calling OpenAI gpt-4.1-mini...\n",
      "✓ OpenAI gpt-4.1-mini: It’s completely normal to feel uncertain about a decision. Would you like to share more about what y\n",
      "\n",
      "Meta Llama 3.1 8B:\n",
      "  Status: success\n",
      "  Response: Could you please provide more context or information about the decision you're considering? What are your concerns or reservations about it? I'd be happy to help you weigh the pros and cons or explore different perspectives.\n",
      "\n",
      "OpenAI gpt-4.1-mini:\n",
      "  Status: success\n",
      "  Response: It’s completely normal to feel uncertain about a decision. Would you like to share more about what you’re deciding on? Sometimes talking it through can help clarify your thoughts.\n"
     ]
    }
   ],
   "source": [
    "# Get responses from models\n",
    "user_input = \"I'm not sure about this decision.\"\n",
    "responses = get_all_github_model_responses(user_input, max_tokens=100)\n",
    "# Display results\n",
    "for resp in responses:\n",
    "    print(f\"\\n{resp['model']}:\")\n",
    "    print(f\"  Status: {resp['status']}\")\n",
    "    if resp['status'] == 'success':\n",
    "        print(f\"  Response: {resp['response']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {resp.get('error', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282a796",
   "metadata": {},
   "source": [
    "### Process All Questions from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_questions_with_model(model_id, model_name=None, sleep_duration=1.0, max_questions=None):\n",
    "    \"\"\"\n",
    "    Process questions from database, send to model, and store responses.\n",
    "    \n",
    "    Output dictionary structure:\n",
    "        {\n",
    "            'Control': [{'prompt': '...', 'response': '...'}, ...],\n",
    "            'Formal': [{'prompt': '...', 'response': '...'}, ...],\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model_id\n",
    "    \n",
    "    emotion_categories = [\n",
    "        'Control', 'Formal', 'Casual', 'Confident', \n",
    "        'Hesitant', 'Analytical', 'Emotional', 'Optimistic', 'Pessimistic'\n",
    "    ]\n",
    "    \n",
    "    results = {category: [] for category in emotion_categories}\n",
    "    \n",
    "    with open('questions_database.json', 'r') as f:\n",
    "        questions_db = json.load(f)\n",
    "    \n",
    "    if max_questions:\n",
    "        questions_db = questions_db[:max_questions]\n",
    "    \n",
    "    total_requests = len(questions_db) * len(emotion_categories)\n",
    "    print(f\"Processing {len(questions_db)} questions with {len(emotion_categories)} emotion categories\")\n",
    "    print(f\"Total API calls: {total_requests}\")\n",
    "    print(f\"Estimated time: {total_requests * sleep_duration / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    for q_idx, question_entry in enumerate(tqdm(questions_db, desc=\"Questions\")):\n",
    "        for category in emotion_categories:\n",
    "            if category not in question_entry:\n",
    "                print(f\"Warning: Category '{category}' not found in question {q_idx}\")\n",
    "                continue\n",
    "            \n",
    "            prompt = question_entry[category]\n",
    "            \n",
    "            response_data = get_github_model_response(\n",
    "                model_name=model_name,\n",
    "                model_id=model_id,\n",
    "                prompt=prompt,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            if response_data['status'] == 'success':\n",
    "                results[category].append({\n",
    "                    'prompt': prompt,\n",
    "                    'response': response_data['response']\n",
    "                })\n",
    "            else:\n",
    "                error_msg = response_data.get('error', 'Unknown error')\n",
    "                print(f\"\\nError for question {q_idx}, category {category}: {error_msg}\")\n",
    "                results[category].append({\n",
    "                    'prompt': prompt,\n",
    "                    'response': None,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "            \n",
    "            time.sleep(sleep_duration)\n",
    "    \n",
    "    print(f\"\\n✓ Completed processing!\")\n",
    "    print(f\"Results summary:\")\n",
    "    for category, entries in results.items():\n",
    "        successful = sum(1 for e in entries if e.get('response') is not None)\n",
    "        print(f\"  {category}: {successful}/{len(entries)} successful\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb301be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing Meta Llama 3.1 8B\")\n",
    "print(\"Requests per minute: 15\")\n",
    "results = process_questions_with_model(model_id=MODELS[\"Meta Llama 3.1 8B\"], sleep_duration=4.5)\n",
    "with open(f'results_llama.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2488dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing OpenAI gpt-4.1-mini\")\n",
    "print(\"Requests per minute: ???\")\n",
    "results = process_questions_with_model(model_id=MODELS[\"OpenAI gpt-4.1-mini\"], sleep_duration=???)\n",
    "with open(f'results_gpt4.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de862d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fixed Unicode escape sequences in results_meta_Meta-Llama-3.1-8B-Instruct.json\n"
     ]
    }
   ],
   "source": [
    "def fix_unicode_in_json_file(filepath):\n",
    "    \"\"\"\n",
    "    Load JSON file and resave it with actual Unicode characters instead of escape sequences.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Fixed Unicode escape sequences in {filepath}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
